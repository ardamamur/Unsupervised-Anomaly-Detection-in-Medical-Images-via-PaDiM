{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import sample\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import wide_resnet50_2, resnet18\n",
    "from fast_ixi import FAST_IXI\n",
    "from utils import read_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# device setup\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "print('Device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'class_names': ['absent_sentum',\n",
       "   'artefacts',\n",
       "   'craniatomy',\n",
       "   'dural',\n",
       "   'ea_mass',\n",
       "   'edema',\n",
       "   'encephalo',\n",
       "   'extra_axial'],\n",
       "  'path': '/home/mamur/TUM/Seminar/dataset',\n",
       "  'resize': 128,\n",
       "  'cropsize': 128,\n",
       "  'batch_size': 1},\n",
       " 'model': {'backbone': 'wide_resnet50_2'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = read_config('config.yaml')\n",
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './results/' + opt['model']['backbone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-trained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mamur/miniconda3/envs/adlm/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mamur/miniconda3/envs/adlm/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Wide_ResNet50_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet50_2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "if opt['model']['backbone'] == 'resnet18':\n",
    "    model = resnet18(pretrained=True, progress=True)\n",
    "    t_d = 448\n",
    "    d = 180\n",
    "\n",
    "elif opt['model']['backbone'] == 'wide_resnet50_2':\n",
    "    model = wide_resnet50_2(pretrained=True, progress=True)\n",
    "    t_d = 1792\n",
    "    d = 550\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone: wide_resnet50_2\n",
      "Input dim size: 1792\n",
      "Output dim size after reduced: 550\n"
     ]
    }
   ],
   "source": [
    "print('Backbone: {}'.format(opt['model']['backbone']))\n",
    "print('Input dim size: {}'.format(t_d))\n",
    "print('Output dim size after reduced: {}'.format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "random.seed(1024)\n",
    "torch.manual_seed(1024)\n",
    "torch.cuda.manual_seed_all(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select randomly choosen dimension to reduce the dimensionality of the feature vector (like PCA)\n",
    "idx = torch.tensor(sample(range(0, t_d), d))\n",
    "# set model's intermediate outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f7a5663b250>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = []\n",
    "def hook(module, input, output):\n",
    "    outputs.append(output)\n",
    "model.layer1[-1].register_forward_hook(hook)\n",
    "model.layer2[-1].register_forward_hook(hook)\n",
    "model.layer3[-1].register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create path to save the results if not exists\n",
    "os.makedirs(os.path.join(save_dir, 'temp_%s' % opt['model']['backbone']), exist_ok=True)\n",
    "train_feature_filepath = os.path.join(save_dir, 'temp_%s' % opt['model']['backbone'], 'train_%s.pkl' % 'brainmri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABkwAAAMzCAYAAADkvj7hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAzwUlEQVR4nO3dbWyd9Xn48ctxsA0qNrAszsNMM2gpbYGEJsQzFCEmr5FAaXkxNYUqySIeRpshGmsrCQ9xKS1mDFCkEhqRwuiLsqRFgKomCqNeo4qSKWoeJDqeRANNVtUmWYfNQhuDff9fVHX/bhLI7cY+Plyfj3Re5O59+/xO9SO5L319zqkpiqIIAAAAAACAxCZVegEAAAAAAACVJpgAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6ZUOJj/+8Y9j4cKFMWPGjKipqYknn3zyPa/ZunVrfOITn4j6+vr40Ic+FI888sgolgoAADDxmZkAAKA6lQ4mBw8ejNmzZ8fatWuP6fxXX301Lr/88rj00ktj9+7d8aUvfSmuueaaeOqpp0ovFgAAYKIzMwEAQHWqKYqiGPXFNTXxxBNPxBVXXHHUc2666abYtGlT/OxnPxs+9rnPfS7eeOON2LJly2ifGgAAYMIzMwEAQPWYPNZPsG3btmhvbx9xbMGCBfGlL33pqNccOnQoDh06NPznoaGh+PWvfx1/9md/FjU1NWO1VAAAmBCKoog333wzZsyYEZMm+drB9zszEwAAlDcWc9OYB5Oenp5obm4ecay5uTn6+/vjN7/5TZx44omHXdPV1RW33377WC8NAAAmtH379sVf/MVfVHoZjDEzEwAAjN7xnJvGPJiMxqpVq6Kjo2P4z319fXH66afHvn37orGxsYIrAwCAsdff3x8tLS1x8sknV3opTFBmJgAAshuLuWnMg8m0adOit7d3xLHe3t5obGw84m9KRUTU19dHfX39YccbGxvd/AMAkIaPVsrBzAQAAKN3POemMf9A5La2tuju7h5x7Omnn462traxfmoAAIAJz8wEAAATQ+lg8n//93+xe/fu2L17d0REvPrqq7F79+7Yu3dvRPzureFLliwZPv/666+PPXv2xJe//OV48cUX44EHHojvfve7sWLFiuPzCgAAACYQMxMAAFSn0sHkpz/9aZx//vlx/vnnR0RER0dHnH/++bF69eqIiPjVr341PAhERPzlX/5lbNq0KZ5++umYPXt23HvvvfGtb30rFixYcJxeAgAAwMRhZgIAgOpUUxRFUelFvJf+/v5oamqKvr4+n8cLAMD7nvtfyrJnAADIZizugcf8O0wAAAAAAAAmOsEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0htVMFm7dm3MmjUrGhoaorW1NbZv3/6u569ZsyY+8pGPxIknnhgtLS2xYsWK+O1vfzuqBQMAAEx0ZiYAAKg+pYPJxo0bo6OjIzo7O2Pnzp0xe/bsWLBgQbz++utHPP/RRx+NlStXRmdnZ7zwwgvx0EMPxcaNG+Pmm2/+kxcPAAAw0ZiZAACgOpUOJvfdd19ce+21sWzZsvjYxz4W69ati5NOOikefvjhI57/7LPPxkUXXRRXXXVVzJo1Kz71qU/FlVde+Z6/YQUAAFCNzEwAAFCdSgWTgYGB2LFjR7S3t//hB0yaFO3t7bFt27YjXnPhhRfGjh07hm/29+zZE5s3b47LLrvsqM9z6NCh6O/vH/EAAACY6MxMAABQvSaXOfnAgQMxODgYzc3NI443NzfHiy++eMRrrrrqqjhw4EB88pOfjKIo4p133onrr7/+Xd9e3tXVFbfffnuZpQEAAFScmQkAAKrXqL70vYytW7fGnXfeGQ888EDs3LkzHn/88di0aVPccccdR71m1apV0dfXN/zYt2/fWC8TAACgIsxMAAAwMZR6h8mUKVOitrY2ent7Rxzv7e2NadOmHfGa2267LRYvXhzXXHNNRESce+65cfDgwbjuuuvilltuiUmTDm829fX1UV9fX2ZpAAAAFWdmAgCA6lXqHSZ1dXUxd+7c6O7uHj42NDQU3d3d0dbWdsRr3nrrrcNu8GtrayMioiiKsusFAACYsMxMAABQvUq9wyQioqOjI5YuXRrz5s2L+fPnx5o1a+LgwYOxbNmyiIhYsmRJzJw5M7q6uiIiYuHChXHffffF+eefH62trfHKK6/EbbfdFgsXLhweAgAAAN4vzEwAAFCdSgeTRYsWxf79+2P16tXR09MTc+bMiS1btgx/qeHevXtH/HbUrbfeGjU1NXHrrbfGL3/5y/jzP//zWLhwYXz9618/fq8CAABggjAzAQBAdaopquA93v39/dHU1BR9fX3R2NhY6eUAAMCYcv9LWfYMAADZjMU9cKnvMAEAAAAAAHg/EkwAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgvVEFk7Vr18asWbOioaEhWltbY/v27e96/htvvBHLly+P6dOnR319fZx11lmxefPmUS0YAABgojMzAQBA9Zlc9oKNGzdGR0dHrFu3LlpbW2PNmjWxYMGCeOmll2Lq1KmHnT8wMBB/8zd/E1OnTo3HHnssZs6cGb/4xS/ilFNOOR7rBwAAmFDMTAAAUJ1qiqIoylzQ2toaF1xwQdx///0RETE0NBQtLS1xww03xMqVKw87f926dfEv//Iv8eKLL8YJJ5wwqkX29/dHU1NT9PX1RWNj46h+BgAAVAv3v9XNzAQAAGNvLO6BS30k18DAQOzYsSPa29v/8AMmTYr29vbYtm3bEa/5/ve/H21tbbF8+fJobm6Oc845J+68884YHBw86vMcOnQo+vv7RzwAAAAmOjMTAABUr1LB5MCBAzE4OBjNzc0jjjc3N0dPT88Rr9mzZ0889thjMTg4GJs3b47bbrst7r333vja17521Ofp6uqKpqam4UdLS0uZZQIAAFSEmQkAAKrXqL70vYyhoaGYOnVqPPjggzF37txYtGhR3HLLLbFu3bqjXrNq1aro6+sbfuzbt2+slwkAAFARZiYAAJgYSn3p+5QpU6K2tjZ6e3tHHO/t7Y1p06Yd8Zrp06fHCSecELW1tcPHPvrRj0ZPT08MDAxEXV3dYdfU19dHfX19maUBAABUnJkJAACqV6l3mNTV1cXcuXOju7t7+NjQ0FB0d3dHW1vbEa+56KKL4pVXXomhoaHhYy+//HJMnz79iDf+AAAA1crMBAAA1av0R3J1dHTE+vXr49vf/na88MIL8YUvfCEOHjwYy5Yti4iIJUuWxKpVq4bP/8IXvhC//vWv48Ybb4yXX345Nm3aFHfeeWcsX778+L0KAACACcLMBAAA1anUR3JFRCxatCj2798fq1evjp6enpgzZ05s2bJl+EsN9+7dG5Mm/aHDtLS0xFNPPRUrVqyI8847L2bOnBk33nhj3HTTTcfvVQAAAEwQZiYAAKhONUVRFJVexHvp7++Ppqam6Ovri8bGxkovBwAAxpT7X8qyZwAAyGYs7oFLfyQXAAAAAADA+41gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOmNKpisXbs2Zs2aFQ0NDdHa2hrbt28/pus2bNgQNTU1ccUVV4zmaQEAAKqCmQkAAKpP6WCycePG6OjoiM7Ozti5c2fMnj07FixYEK+//vq7Xvfaa6/FP/7jP8bFF1886sUCAABMdGYmAACoTqWDyX333RfXXnttLFu2LD72sY/FunXr4qSTToqHH374qNcMDg7G5z//+bj99tvjjDPO+JMWDAAAMJGZmQAAoDqVCiYDAwOxY8eOaG9v/8MPmDQp2tvbY9u2bUe97qtf/WpMnTo1rr766mN6nkOHDkV/f/+IBwAAwERnZgIAgOpVKpgcOHAgBgcHo7m5ecTx5ubm6OnpOeI1zzzzTDz00EOxfv36Y36erq6uaGpqGn60tLSUWSYAAEBFmJkAAKB6jepL34/Vm2++GYsXL47169fHlClTjvm6VatWRV9f3/Bj3759Y7hKAACAyjAzAQDAxDG5zMlTpkyJ2tra6O3tHXG8t7c3pk2bdtj5P//5z+O1116LhQsXDh8bGhr63RNPnhwvvfRSnHnmmYddV19fH/X19WWWBgAAUHFmJgAAqF6l3mFSV1cXc+fOje7u7uFjQ0ND0d3dHW1tbYedf/bZZ8dzzz0Xu3fvHn58+tOfjksvvTR2797tbeMAAMD7ipkJAACqV6l3mEREdHR0xNKlS2PevHkxf/78WLNmTRw8eDCWLVsWERFLliyJmTNnRldXVzQ0NMQ555wz4vpTTjklIuKw4wAAAO8HZiYAAKhOpYPJokWLYv/+/bF69ero6emJOXPmxJYtW4a/1HDv3r0xadKYfjUKAADAhGVmAgCA6lRTFEVR6UW8l/7+/mhqaoq+vr5obGys9HIAAGBMuf+lLHsGAIBsxuIe2K81AQAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkJ5gAgAAAAAApCeYAAAAAAAA6QkmAAAAAABAeoIJAAAAAACQnmACAAAAAACkJ5gAAAAAAADpCSYAAAAAAEB6ggkAAAAAAJCeYAIAAAAAAKQnmAAAAAAAAOkJJgAAAAAAQHqCCQAAAAAAkN6ogsnatWtj1qxZ0dDQEK2trbF9+/ajnrt+/fq4+OKL49RTT41TTz012tvb3/V8AACAamdmAgCA6lM6mGzcuDE6Ojqis7Mzdu7cGbNnz44FCxbE66+/fsTzt27dGldeeWX86Ec/im3btkVLS0t86lOfil/+8pd/8uIBAAAmGjMTAABUp5qiKIoyF7S2tsYFF1wQ999/f0REDA0NRUtLS9xwww2xcuXK97x+cHAwTj311Lj//vtjyZIlx/Sc/f390dTUFH19fdHY2FhmuQAAUHXc/1Y3MxMAAIy9sbgHLvUOk4GBgdixY0e0t7f/4QdMmhTt7e2xbdu2Y/oZb731Vrz99ttx2mmnHfWcQ4cORX9//4gHAADARGdmAgCA6lUqmBw4cCAGBwejubl5xPHm5ubo6ek5pp9x0003xYwZM0YMEH+sq6srmpqahh8tLS1llgkAAFARZiYAAKheo/rS99G66667YsOGDfHEE09EQ0PDUc9btWpV9PX1DT/27ds3jqsEAACoDDMTAABUzuQyJ0+ZMiVqa2ujt7d3xPHe3t6YNm3au157zz33xF133RU//OEP47zzznvXc+vr66O+vr7M0gAAACrOzAQAANWr1DtM6urqYu7cudHd3T18bGhoKLq7u6Otre2o1919991xxx13xJYtW2LevHmjXy0AAMAEZmYCAIDqVeodJhERHR0dsXTp0pg3b17Mnz8/1qxZEwcPHoxly5ZFRMSSJUti5syZ0dXVFRER//zP/xyrV6+ORx99NGbNmjX8ub0f+MAH4gMf+MBxfCkAAACVZ2YCAIDqVDqYLFq0KPbv3x+rV6+Onp6emDNnTmzZsmX4Sw337t0bkyb94Y0r3/zmN2NgYCD+9m//dsTP6ezsjK985St/2uoBAAAmGDMTAABUp5qiKIpKL+K99Pf3R1NTU/T19UVjY2OllwMAAGPK/S9l2TMAAGQzFvfApb7DBAAAAAAA4P1IMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0RhVM1q5dG7NmzYqGhoZobW2N7du3v+v53/ve9+Lss8+OhoaGOPfcc2Pz5s2jWiwAAEA1MDMBAED1KR1MNm7cGB0dHdHZ2Rk7d+6M2bNnx4IFC+L1118/4vnPPvtsXHnllXH11VfHrl274oorrogrrrgifvazn/3JiwcAAJhozEwAAFCdaoqiKMpc0NraGhdccEHcf//9ERExNDQULS0tccMNN8TKlSsPO3/RokVx8ODB+MEPfjB87K/+6q9izpw5sW7dumN6zv7+/mhqaoq+vr5obGwss1wAAKg67n+rm5kJAADG3ljcA08uc/LAwEDs2LEjVq1aNXxs0qRJ0d7eHtu2bTviNdu2bYuOjo4RxxYsWBBPPvnkUZ/n0KFDcejQoeE/9/X1RcTv/g8AAID3u9/f95b83SYmADMTAACMj7GYm0oFkwMHDsTg4GA0NzePON7c3BwvvvjiEa/p6ek54vk9PT1HfZ6urq64/fbbDzve0tJSZrkAAFDV/ud//ieampoqvQxKMDMBAMD4Op5zU6lgMl5WrVo14jes3njjjfjgBz8Ye/fuNTByTPr7+6OlpSX27dvnIwk4JvYMZdgvlGXPUFZfX1+cfvrpcdppp1V6KUxQZib+VP5toix7hrLsGcqyZyhrLOamUsFkypQpUVtbG729vSOO9/b2xrRp0454zbRp00qdHxFRX18f9fX1hx1vamryHwulNDY22jOUYs9Qhv1CWfYMZU2aNKnSS6AkMxPVxr9NlGXPUJY9Q1n2DGUdz7mp1E+qq6uLuXPnRnd39/CxoaGh6O7ujra2tiNe09bWNuL8iIinn376qOcDAABUKzMTAABUr9IfydXR0RFLly6NefPmxfz582PNmjVx8ODBWLZsWURELFmyJGbOnBldXV0REXHjjTfGJZdcEvfee29cfvnlsWHDhvjpT38aDz744PF9JQAAABOAmQkAAKpT6WCyaNGi2L9/f6xevTp6enpizpw5sWXLluEvKdy7d++It8BceOGF8eijj8att94aN998c3z4wx+OJ598Ms4555xjfs76+vro7Ow84lvO4UjsGcqyZyjDfqEse4ay7JnqZmaiGtgzlGXPUJY9Q1n2DGWNxZ6pKYqiOG4/DQAAAAAAoAr5FkkAAAAAACA9wQQAAAAAAEhPMAEAAAAAANITTAAAAAAAgPQmTDBZu3ZtzJo1KxoaGqK1tTW2b9/+rud/73vfi7PPPjsaGhri3HPPjc2bN4/TSpkoyuyZ9evXx8UXXxynnnpqnHrqqdHe3v6ee4z3l7J/x/zehg0boqamJq644oqxXSATTtk988Ybb8Ty5ctj+vTpUV9fH2eddZZ/m5Ipu2fWrFkTH/nIR+LEE0+MlpaWWLFiRfz2t78dp9VSaT/+8Y9j4cKFMWPGjKipqYknn3zyPa/ZunVrfOITn4j6+vr40Ic+FI888siYr5OJxcxEWWYmyjI3UZa5ibLMTRyris1MxQSwYcOGoq6urnj44YeL//qv/yquvfba4pRTTil6e3uPeP5PfvKTora2trj77ruL559/vrj11luLE044oXjuuefGeeVUStk9c9VVVxVr164tdu3aVbzwwgvF3/3d3xVNTU3Ff//3f4/zyqmEsvvl91599dVi5syZxcUXX1x85jOfGZ/FMiGU3TOHDh0q5s2bV1x22WXFM888U7z66qvF1q1bi927d4/zyqmUsnvmO9/5TlFfX1985zvfKV599dXiqaeeKqZPn16sWLFinFdOpWzevLm45ZZbiscff7yIiOKJJ5541/P37NlTnHTSSUVHR0fx/PPPF9/4xjeK2traYsuWLeOzYCrOzERZZibKMjdRlrmJssxNlFGpmWlCBJP58+cXy5cvH/7z4OBgMWPGjKKrq+uI53/2s58tLr/88hHHWltbi7//+78f03UycZTdM3/snXfeKU4++eTi29/+9lgtkQlkNPvlnXfeKS688MLiW9/6VrF06VI3/smU3TPf/OY3izPOOKMYGBgYryUywZTdM8uXLy/++q//esSxjo6O4qKLLhrTdTIxHcvN/5e//OXi4x//+IhjixYtKhYsWDCGK2MiMTNRlpmJssxNlGVuoixzE6M1njNTxT+Sa2BgIHbs2BHt7e3DxyZNmhTt7e2xbdu2I16zbdu2EedHRCxYsOCo5/P+Mpo988feeuutePvtt+O0004bq2UyQYx2v3z1q1+NqVOnxtVXXz0ey2QCGc2e+f73vx9tbW2xfPnyaG5ujnPOOSfuvPPOGBwcHK9lU0Gj2TMXXnhh7NixY/jt53v27InNmzfHZZddNi5rpvq4/83NzERZZibKMjdRlrmJssxNjLXjdf87+XguajQOHDgQg4OD0dzcPOJ4c3NzvPjii0e8pqen54jn9/T0jNk6mThGs2f+2E033RQzZsw47D8i3n9Gs1+eeeaZeOihh2L37t3jsEImmtHsmT179sR//Md/xOc///nYvHlzvPLKK/HFL34x3n777ejs7ByPZVNBo9kzV111VRw4cCA++clPRlEU8c4778T1118fN99883gsmSp0tPvf/v7++M1vfhMnnnhihVbGeDAzUZaZibLMTZRlbqIscxNj7XjNTBV/hwmMt7vuuis2bNgQTzzxRDQ0NFR6OUwwb775ZixevDjWr18fU6ZMqfRyqBJDQ0MxderUePDBB2Pu3LmxaNGiuOWWW2LdunWVXhoT1NatW+POO++MBx54IHbu3BmPP/54bNq0Ke64445KLw0AzEy8J3MTo2FuoixzE5VQ8XeYTJkyJWpra6O3t3fE8d7e3pg2bdoRr5k2bVqp83l/Gc2e+b177rkn7rrrrvjhD38Y55133lgukwmi7H75+c9/Hq+99losXLhw+NjQ0FBEREyePDleeumlOPPMM8d20VTUaP6OmT59epxwwglRW1s7fOyjH/1o9PT0xMDAQNTV1Y3pmqms0eyZ2267LRYvXhzXXHNNRESce+65cfDgwbjuuuvilltuiUmT/E4LIx3t/rexsdG7SxIwM1GWmYmyzE2UZW6iLHMTY+14zUwV31V1dXUxd+7c6O7uHj42NDQU3d3d0dbWdsRr2traRpwfEfH0008f9XzeX0azZyIi7r777rjjjjtiy5YtMW/evPFYKhNA2f1y9tlnx3PPPRe7d+8efnz605+OSy+9NHbv3h0tLS3juXwqYDR/x1x00UXxyiuvDA+JEREvv/xyTJ8+3U1/AqPZM2+99dZhN/e/Hxx/9312MJL739zMTJRlZqIscxNlmZsoy9zEWDtu97+lviJ+jGzYsKGor68vHnnkkeL5558vrrvuuuKUU04penp6iqIoisWLFxcrV64cPv8nP/lJMXny5OKee+4pXnjhhaKzs7M44YQTiueee65SL4FxVnbP3HXXXUVdXV3x2GOPFb/61a+GH2+++WalXgLjqOx++WNLly4tPvOZz4zTapkIyu6ZvXv3FieffHLxD//wD8VLL71U/OAHPyimTp1afO1rX6vUS2Ccld0znZ2dxcknn1z827/9W7Fnz57i3//934szzzyz+OxnP1upl8A4e/PNN4tdu3YVu3btKiKiuO+++4pdu3YVv/jFL4qiKIqVK1cWixcvHj5/z549xUknnVT80z/9U/HCCy8Ua9euLWpra4stW7ZU6iUwzsxMlGVmoixzE2WZmyjL3EQZlZqZJkQwKYqi+MY3vlGcfvrpRV1dXTF//vziP//zP4f/t0suuaRYunTpiPO/+93vFmeddVZRV1dXfPzjHy82bdo0zium0srsmQ9+8INFRBz26OzsHP+FUxFl/475/7nxz6nsnnn22WeL1tbWor6+vjjjjDOKr3/968U777wzzqumksrsmbfffrv4yle+Upx55plFQ0ND0dLSUnzxi18s/vd//3f8F05F/OhHPzrivcnv98nSpUuLSy655LBr5syZU9TV1RVnnHFG8a//+q/jvm4qy8xEWWYmyjI3UZa5ibLMTRyrSs1MNUXh/UsAAAAAAEBuFf8OEwAAAAAAgEoTTAAAAAAAgPQEEwAAAAAAID3BBAAAAAAASE8wAQAAAAAA0hNMAAAAAACA9AQTAAAAAAAgPcEEAAAAAABITzABAAAAAADSE0wAAAAAAID0BBMAAAAAACA9wQQAAAAAAEjv/wEg7BzH6TfmsgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 2000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "fig_img_rocauc = ax[0]\n",
    "fig_pixel_rocauc = ax[1]\n",
    "total_roc_auc = []\n",
    "total_pixel_roc_auc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "train_dataset = FAST_IXI(opt, is_train=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=opt['dataset']['batch_size'], pin_memory=True)\n",
    "\n",
    "test_dataset = FAST_IXI(opt, is_train=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=opt['dataset']['batch_size'], pin_memory=True)\n",
    "\n",
    "train_outputs = OrderedDict([('layer1', []), ('layer2', []), ('layer3', [])])\n",
    "test_outputs = OrderedDict([('layer1', []), ('layer2', []), ('layer3', [])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 711\n",
      "test_size: 111\n"
     ]
    }
   ],
   "source": [
    "print('train size:', len(train_dataloader))\n",
    "print('test_size:', len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Class Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_concat(x, y):\n",
    "    B, C1, H1, W1 = x.size()\n",
    "    _, C2, H2, W2 = y.size()\n",
    "    s = int(H1 / H2)\n",
    "    x = F.unfold(x, kernel_size=s, dilation=1, stride=s)\n",
    "    x = x.view(B, C1, -1, H2, W2)\n",
    "    z = torch.zeros(B, C1 + C2, x.size(2), H2, W2)\n",
    "    for i in range(x.size(2)):\n",
    "        z[:, :, i, :, :] = torch.cat((x[:, :, i, :, :], y), 1)\n",
    "    z = z.view(B, -1, H2 * W2)\n",
    "    z = F.fold(z, kernel_size=s, output_size=(H1, W1), stride=s)\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1', []), ('layer2', []), ('layer3', [])])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from each layer of the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Make optional to extract feature method (read from the file or extract from the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train feature extraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | train | brainmri |: 100%|██████████| 711/711 [00:10<00:00, 69.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first layer shape: torch.Size([711, 256, 32, 32])\n",
      "second layer shape: torch.Size([711, 512, 16, 16])\n",
      "third layer shape: torch.Size([711, 1024, 8, 8])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if not os.path.exists(train_feature_filepath):\n",
    "    print('train feature extraction')\n",
    "    for (img, label, mask) in tqdm(train_dataloader, '| feature extraction | train | %s |' % 'brainmri'):\n",
    "        img = img.to(device)\n",
    "        with torch.no_grad():\n",
    "            _ = model(img)\n",
    "        for key, value in zip(train_outputs.keys(), outputs):\n",
    "            train_outputs[key].append(value.cpu().detach())\n",
    "        # initialize hook outputs\n",
    "        outputs = []        \n",
    "    for key, value in train_outputs.items():\n",
    "        train_outputs[key] = torch.cat(value, 0)\n",
    "\n",
    "    print('first layer shape:', train_outputs['layer1'].shape)\n",
    "    print('second layer shape:', train_outputs['layer2'].shape)\n",
    "    print('third layer shape:', train_outputs['layer3'].shape)\n",
    "\n",
    "    # Embedding concat\n",
    "    embedding_vectors = train_outputs['layer1'] # get the maximum size of the embedding vectors\n",
    "   \n",
    "    \"\"\"\n",
    "    Rresearchers conceptually divide the input image into a grid based on the resolution of the largest activation map—typically\n",
    "    the first layer of the pre-trained CNN. This way, each grid position, denoted as (i,j), \n",
    "    is associated with a unique embedding vector that represents the collective activation vectors for that particular image patch.\n",
    "    \"\"\"\n",
    "    for layer_name in ['layer2', 'layer3']:\n",
    "        embedding_vectors = embedding_concat(embedding_vectors, train_outputs[layer_name])\n",
    "\n",
    "    # randomly select d dimension\n",
    "    print('randomly select %d dimension' % d)\n",
    "    embedding_vectors = torch.index_select(embedding_vectors, 1, idx)\n",
    "\n",
    "    B, C, H, W = embedding_vectors.size() # Get the shape of the embedding vectors which is same with the first layer of the pretrained model\n",
    "    print('embedding_vectors shape:', embedding_vectors.shape)\n",
    "    embedding_vectors = embedding_vectors.view(B, C, H * W)\n",
    "\n",
    "    # calculate multivariate Gaussian distribution\n",
    "    mean = torch.mean(embedding_vectors, dim=0).numpy()\n",
    "    cov = torch.zeros(C, C, H * W).numpy()\n",
    "    I = np.identity(C)\n",
    "\n",
    "    # calculate mean, cov and inverse covariance matrix for each patch position at Xij \n",
    "    # (each patch position (i,j) is associated with a unique embedding vector)\n",
    "    for i in range(H * W):\n",
    "        # Xij = embedding_vectors[:, :, i].numpy()\n",
    "        cov[:, :, i] = np.cov(embedding_vectors[:, :, i].numpy(), rowvar=False) + 0.01 * I\n",
    "\n",
    "    # save learned distribution\n",
    "    train_outputs = [mean, cov]\n",
    "    with open(train_feature_filepath, 'wb') as f:\n",
    "        pickle.dump(train_outputs, f)\n",
    "else:\n",
    "    # load train set features\n",
    "    with open(train_feature_filepath, 'rb') as f:\n",
    "        train_outputs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PaDiM at Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_list = []\n",
    "gt_mask_list = []\n",
    "test_imgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | brainmri |: 100%|██████████| 111/111 [00:00<00:00, 111.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first layer shape: torch.Size([111, 64, 32, 32])\n",
      "second layer shape: torch.Size([111, 128, 16, 16])\n",
      "third layer shape: torch.Size([111, 256, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# extract test set features\n",
    "for (img, label, mask) in tqdm(test_dataloader, '| feature extraction | test | %s |' % 'brainmri'):\n",
    "    test_imgs.extend(img.cpu().detach().numpy())\n",
    "    gt_list.extend(label.cpu().detach().numpy())\n",
    "    gt_mask_list.extend(mask.cpu().detach().numpy())\n",
    "\n",
    "    # get the model prediction\n",
    "    with torch.no_grad():\n",
    "        _ = model(img.to(device))\n",
    "    \n",
    "    # get intermediate outputs\n",
    "    for key, value in zip(test_outputs.keys(), outputs):\n",
    "        test_outputs[key].append(value.cpu().detach())\n",
    "\n",
    "    # initialize hook outputs\n",
    "    outputs = []\n",
    "\n",
    "for key, value in test_outputs.items():\n",
    "    test_outputs[key] = torch.cat(value, 0)\n",
    "\n",
    "print('first layer shape:', test_outputs['layer1'].shape)\n",
    "print('second layer shape:', test_outputs['layer2'].shape)\n",
    "print('third layer shape:', test_outputs['layer3'].shape)\n",
    "\n",
    "# Embedding concat\n",
    "embedding_vectors = test_outputs['layer1']\n",
    "for layer_name in ['layer2', 'layer3']:\n",
    "    embedding_vectors = embedding_concat(embedding_vectors, test_outputs[layer_name])\n",
    "\n",
    "# randomly select d dimension\n",
    "embedding_vectors = torch.index_select(embedding_vectors, 1, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_vectors shape: torch.Size([111, 180, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# calculate mahalanobis distance between train_outputs to give anomaly score to each patch position of the test images\n",
    "B, C, H, W = embedding_vectors.size()\n",
    "print('embedding_vectors shape:', embedding_vectors.shape)\n",
    "\n",
    "embedding_vectors = embedding_vectors.view(B, C, H * W).numpy()\n",
    "dist_list = []\n",
    "for i in range(H * W):\n",
    "    mean = train_outputs[0][:, i]\n",
    "    conv_inv = np.linalg.inv(train_outputs[1][:, :, i])\n",
    "    dist = [mahalanobis(sample[:, i], mean, conv_inv) for sample in embedding_vectors]\n",
    "    dist_list.append(dist)\n",
    "dist_list = np.array(dist_list).transpose(1, 0).reshape(B, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample to image size to get anomaly score map\n",
    "dist_list = torch.tensor(dist_list)\n",
    "score_map = F.interpolate(dist_list.unsqueeze(1), size=img.size(2), mode='bilinear',\n",
    "                            align_corners=False).squeeze().numpy()\n",
    "\n",
    "# apply gaussian smoothing on the score map\n",
    "for i in range(score_map.shape[0]):\n",
    "    score_map[i] = gaussian_filter(score_map[i], sigma=4)\n",
    "\n",
    "# Normalize the score map\n",
    "max_score = score_map.max()\n",
    "min_score = score_map.min()\n",
    "scores = (score_map - min_score) / (max_score - min_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import denormalization, plot_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image ROCAUC: 0.823\n"
     ]
    }
   ],
   "source": [
    "# calculate image-level ROC AUC score\n",
    "img_scores = scores.reshape(scores.shape[0], -1).max(axis=1)\n",
    "gt_list = np.asarray(gt_list)\n",
    "fpr, tpr, _ = roc_curve(gt_list, img_scores)\n",
    "img_roc_auc = roc_auc_score(gt_list, img_scores)\n",
    "total_roc_auc.append(img_roc_auc)\n",
    "fig_img_rocauc.plot(fpr, tpr, label='%s img_ROCAUC: %.3f' % ('brainmri', img_roc_auc))\n",
    "print('image ROCAUC: %.3f' % (img_roc_auc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: 0.283\n"
     ]
    }
   ],
   "source": [
    "# get optimal threshold\n",
    "gt_mask = np.asarray(gt_mask_list)\n",
    "precision, recall, thresholds = precision_recall_curve(gt_mask.flatten(), scores.flatten())\n",
    "a = 2 * precision * recall\n",
    "b = precision + recall\n",
    "f1 = np.divide(a, b, out=np.zeros_like(a), where=b != 0)\n",
    "threshold = thresholds[np.argmax(f1)]\n",
    "print('threshold: %.3f' % (threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel ROCAUC: 0.882\n"
     ]
    }
   ],
   "source": [
    "# calculate per-pixel level ROCAUC\n",
    "fpr, tpr, _ = roc_curve(gt_mask.flatten(), scores.flatten())\n",
    "per_pixel_rocauc = roc_auc_score(gt_mask.flatten(), scores.flatten())\n",
    "total_pixel_roc_auc.append(per_pixel_rocauc)\n",
    "print('pixel ROCAUC: %.3f' % (per_pixel_rocauc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_pixel_rocauc.plot(fpr, tpr, label='%s ROCAUC: %.3f' % ('brainmri', per_pixel_rocauc))\n",
    "save_dir = save_dir + '/pictures_' + opt['model']['backbone']\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "plot_fig(test_imgs, scores, gt_mask_list, threshold, save_dir, 'brainmri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROCAUC: 0.823\n",
      "Average pixel ROCUAC: 0.882\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Average ROCAUC: %.3f' % np.mean(total_roc_auc))\n",
    "fig_img_rocauc.title.set_text('Average image ROCAUC: %.3f' % np.mean(total_roc_auc))\n",
    "fig_img_rocauc.legend(loc=\"lower right\")\n",
    "\n",
    "print('Average pixel ROCUAC: %.3f' % np.mean(total_pixel_roc_auc))\n",
    "fig_pixel_rocauc.title.set_text('Average pixel ROCAUC: %.3f' % np.mean(total_pixel_roc_auc))\n",
    "fig_pixel_rocauc.legend(loc=\"lower right\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(save_dir, 'roc_curve.png'), dpi=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
