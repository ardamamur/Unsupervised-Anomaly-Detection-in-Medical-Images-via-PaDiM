{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "591667fe-8496-444b-b076-ea2cfeb7439c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mamur/miniconda3/envs/adlm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from random import sample\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f1718b-a3ad-4529-9a6d-16ef5aeefce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# device setup\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "print('Device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9524da8-d71d-4ca6-a696-1c1739486c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "seed = 1024\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5da45b62-d5a4-4d53-ab1e-55e2d74b945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'config.yaml'\n",
    "opt = read_config(config_path)\n",
    "experiment_path = opt['dataset']['save_dir'] + '/' + opt['model']['backbone']\n",
    "opt['model']['experiment_path'] = experiment_path\n",
    "\n",
    "os.makedirs(os.path.join(experiment_path, 'normal_embeddings'), exist_ok=True)\n",
    "train_feature_filepath = os.path.join(experiment_path, 'normal_embeddings', 'train_%s.pkl' % opt['dataset']['name'])\n",
    "opt['model']['train_feature_filepath'] = train_feature_filepath\n",
    "\n",
    "pic_save_path = os.path.join(experiment_path, 'pictures')\n",
    "os.makedirs(pic_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bb0ce40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'ann_path': './data/splits',\n",
       "  'path': './data',\n",
       "  'save_dir': './results',\n",
       "  'name': 'fast_ixi',\n",
       "  'target_size': [128, 128],\n",
       "  'batch_size': 1},\n",
       " 'model': {'backbone': 'resnet18',\n",
       "  'target_dimension': 448,\n",
       "  'output_dimension': 180,\n",
       "  'experiment_path': './results/resnet18',\n",
       "  'train_feature_filepath': './results/resnet18/normal_embeddings/train_fast_ixi.pkl'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845774d1-b9f8-485f-a3c7-b3ee8c2bac88",
   "metadata": {},
   "source": [
    "### Load pre-trained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23ded272-6ff7-4896-971f-61e8f832ffc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mamur/miniconda3/envs/adlm/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mamur/miniconda3/envs/adlm/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone: resnet18\n",
      "Input dim size: 448\n",
      "Output dim size after reduced: 180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pretrained CNN\n",
    "model, t_d, d = load_pretrained_CNN(opt)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d57085d9-598b-4c5a-9ce1-6b14ae1a949b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fa7f43c6970>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select randomly choosen dimension to reduce the dimensionality of the feature vector (like PCA)\n",
    "idx = torch.tensor(sample(range(0, t_d), d))\n",
    "# initialize the intermadiate outputs\n",
    "outputs = []\n",
    "def hook(module, input, output):\n",
    "    outputs.append(output)\n",
    "model.layer1[-1].register_forward_hook(hook)\n",
    "model.layer2[-1].register_forward_hook(hook)\n",
    "model.layer3[-1].register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de9b4cf-197c-4367-929c-460cd1955f42",
   "metadata": {},
   "source": [
    "### Learning Normal Class Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85adc64f-155a-4368-a308-9490dc233918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 581 IXI images and 130 fastMRI images for training. Using 15 images for validation.\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = load_train_dataset(opt).train_dataloader()\n",
    "learned_representation = OrderedDict([('layer1', []), ('layer2', []), ('layer3', [])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a77e7d2-6dfe-4f0a-a377-2f3e122d0f03",
   "metadata": {},
   "source": [
    "#### Extract embeddings fromm the train dataset and save it as Multivariate Gaussian Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "719c93f4-6675-417c-a64e-442dc6cd24f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | train | fast_ixi |: 711it [00:04, 174.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first layer shape: torch.Size([711, 64, 32, 32])\n",
      "second layer shape: torch.Size([711, 128, 16, 16])\n",
      "third layer shape: torch.Size([711, 256, 8, 8])\n",
      "randomly select 180 dimension\n",
      "embedding_vectors shape: torch.Size([711, 180, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(train_feature_filepath):    \n",
    "# for each batch in the dataloader (use tqdm bar), train dataloader get item returns only x\n",
    "    for batch_idx, img in tqdm(enumerate(train_dataloader), '| feature extraction | train | %s |' % opt['dataset']['name']):\n",
    "        img = img.to(device)\n",
    "        with torch.no_grad():\n",
    "            _ = model(img)\n",
    "        for key, value in zip(learned_representation.keys(), outputs):\n",
    "            learned_representation[key].append(value.cpu().detach())\n",
    "        # initialize hook outputs\n",
    "        outputs = []\n",
    "\n",
    "    for key, value in learned_representation.items():\n",
    "        learned_representation[key] = torch.cat(value, 0)\n",
    "\n",
    "    print('first layer shape:', learned_representation['layer1'].shape)\n",
    "    print('second layer shape:', learned_representation['layer2'].shape)\n",
    "    print('third layer shape:', learned_representation['layer3'].shape)\n",
    "    # Embedding concat\n",
    "    embedding_vectors = learned_representation['layer1'] # get the maximum size of the embedding vectors\n",
    "    \"\"\"\n",
    "    Rresearchers conceptually divide the input image into a grid based on the resolution of the largest activation map—typically\n",
    "    the first layer of the pre-trained CNN. This way, each grid position, denoted as (i,j), \n",
    "    is associated with a unique embedding vector that represents the collective activation vectors for that particular image patch.\n",
    "    \"\"\"\n",
    "    for layer_name in ['layer2', 'layer3']:\n",
    "        embedding_vectors = embedding_concat(embedding_vectors, learned_representation[layer_name])\n",
    "\n",
    "    # randomly select d dimension\n",
    "    print('randomly select %d dimension' % opt['model']['output_dimension'])\n",
    "    embedding_vectors = torch.index_select(embedding_vectors, 1, idx)\n",
    "\n",
    "    B, C, H, W = embedding_vectors.size() # Get the shape of the embedding vectors which is same with the first layer of the pretrained model\n",
    "    print('embedding_vectors shape:', embedding_vectors.shape)\n",
    "    embedding_vectors = embedding_vectors.view(B, C, H * W)\n",
    "\n",
    "    # calculate multivariate Gaussian distribution\n",
    "    mean = torch.mean(embedding_vectors, dim=0).numpy()\n",
    "    cov = torch.zeros(C, C, H * W).numpy()\n",
    "    I = np.identity(C)\n",
    "\n",
    "    # calculate mean, cov and inverse covariance matrix for each patch position at Xij \n",
    "    # (each patch position (i,j) is associated with a unique embedding vector)\n",
    "    for i in range(H * W):\n",
    "        # Xij = embedding_vectors[:, :, i].numpy()\n",
    "        cov[:, :, i] = np.cov(embedding_vectors[:, :, i].numpy(), rowvar=False) + 0.01 * I\n",
    "\n",
    "    # save learned distribution\n",
    "    learned_representation = [mean, cov]\n",
    "    with open(train_feature_filepath, 'wb') as f:\n",
    "        pickle.dump(learned_representation, f)\n",
    "else:\n",
    "    with open(train_feature_filepath, 'rb') as f:\n",
    "        learned_representation = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899a9aa-d308-4850-94fa-cf3ddd57746f",
   "metadata": {},
   "source": [
    "### Evaluate on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd39671a-2648-4677-8212-6c319030f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import get_all_test_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e21e81c7-945e-4282-951c-1833eac89c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloaders = get_all_test_dataloaders( opt['dataset']['ann_path'],  opt['dataset']['target_size'], opt['dataset']['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67bc7be3-aa1f-40d3-8597-db0e013d46ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of anomaly classes: 17\n"
     ]
    }
   ],
   "source": [
    "print('number of anomaly classes:', len(test_dataloaders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0c54767-31c6-4fbd-8eb9-3ba8f456a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = {key : [] for key in test_dataloaders.keys()}\n",
    "all_labels = {key : [] for key in test_dataloaders.keys()}\n",
    "all_pos_masks = {key : [] for key in test_dataloaders.keys()}\n",
    "all_neg_masks = {key : [] for key in test_dataloaders.keys()}\n",
    "all_thresholds = {key : [] for key in test_dataloaders.keys()}\n",
    "all_test_outputs = { key: OrderedDict([('layer1', []), ('layer2', []), ('layer3', [])]) for key in test_dataloaders.keys()}\n",
    "all_embedding_vectors = {key : [] for key in test_dataloaders.keys()}\n",
    "all_scores = {key : [] for key in test_dataloaders.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17661861-9e5c-4484-967f-3a4f0e85a1a3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: absent_septum ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | absent_septum |: 100%|██████████| 1/1 [00:00<00:00, 42.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: artefacts ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | artefacts |: 100%|██████████| 16/16 [00:00<00:00, 117.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: craniatomy ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | craniatomy |: 100%|██████████| 15/15 [00:00<00:00, 131.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: dural ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | dural |: 100%|██████████| 7/7 [00:00<00:00, 128.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: ea_mass ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | ea_mass |: 100%|██████████| 4/4 [00:00<00:00, 121.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: edema ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | edema |: 100%|██████████| 18/18 [00:00<00:00, 134.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: encephalomalacia ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | encephalomalacia |: 100%|██████████| 1/1 [00:00<00:00, 141.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: enlarged_ventricles ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | enlarged_ventricles |: 100%|██████████| 19/19 [00:00<00:00, 140.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: intraventricular ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | intraventricular |: 100%|██████████| 1/1 [00:00<00:00, 139.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: lesions ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | lesions |: 100%|██████████| 22/22 [00:00<00:00, 141.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: mass ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | mass |: 100%|██████████| 22/22 [00:00<00:00, 143.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: posttreatment ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | posttreatment |: 100%|██████████| 44/44 [00:00<00:00, 137.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: resection ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | resection |: 100%|██████████| 10/10 [00:00<00:00, 147.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: sinus ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | sinus |: 100%|██████████| 2/2 [00:00<00:00, 143.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: wml ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | wml |: 100%|██████████| 5/5 [00:00<00:00, 128.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: other ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | other |: 100%|██████████| 5/5 [00:00<00:00, 124.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: normal ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | normal |: 100%|██████████| 30/30 [00:00<00:00, 136.20it/s]\n"
     ]
    }
   ],
   "source": [
    "for anomaly_class in test_dataloaders.keys():\n",
    "    print('******************* DATASET: {} ****************'.format(anomaly_class)) \n",
    "    imgs = []\n",
    "    labels = []\n",
    "    pos_masks = []\n",
    "    neg_masks = []\n",
    "    for (img, label, pos_mask, neg_mask) in tqdm(test_dataloaders[anomaly_class], '| feature extraction | test | %s |' % anomaly_class):\n",
    "        imgs.extend(img.cpu().detach().numpy())\n",
    "        labels.extend(label.cpu().detach().numpy())\n",
    "        pos_masks.extend(pos_mask.cpu().detach().numpy())\n",
    "        neg_masks.extend(neg_mask.cpu().detach().numpy())\n",
    "\n",
    "        # get the model prediction\n",
    "        with torch.no_grad():\n",
    "            _ = model(img.to(device))\n",
    "        # get intermediate outputs\n",
    "        for key, value in zip(all_test_outputs[anomaly_class].keys(), outputs):\n",
    "            all_test_outputs[anomaly_class][key].append(value.cpu().detach())\n",
    "        # initialize hook outputs\n",
    "        outputs = []\n",
    "\n",
    "    for key, value in all_test_outputs[anomaly_class].items():\n",
    "        all_test_outputs[anomaly_class][key] = torch.cat(value, 0)\n",
    "    # Embedding concat\n",
    "    embedding_vectors = all_test_outputs[anomaly_class]['layer1']\n",
    "    for layer_name in ['layer2', 'layer3']:\n",
    "        embedding_vectors = embedding_concat(embedding_vectors, all_test_outputs[anomaly_class][layer_name])\n",
    "    \n",
    "    # randomly select d dimension\n",
    "    embedding_vectors = torch.index_select(embedding_vectors, 1, idx)\n",
    "    #print(embedding_vectors.shape)\n",
    "    \n",
    "\n",
    "    all_images[anomaly_class] = imgs\n",
    "    all_labels[anomaly_class] = labels\n",
    "    all_pos_masks[anomaly_class] = pos_masks\n",
    "    all_neg_masks[anomaly_class] = neg_masks\n",
    "    all_embedding_vectors[anomaly_class] = embedding_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45310ca7-e85e-4985-b27f-83599b8dcdec",
   "metadata": {},
   "source": [
    "#### Get Anomaly Map for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e5e97cb-a1b7-46af-bfa2-a780039fb284",
   "metadata": {},
   "outputs": [],
   "source": [
    "for anomaly_class in test_dataloaders.keys():\n",
    "    #print('******************* DATASET: {} ****************'.format(anomaly_class)) \n",
    "    # calculate mahalanobis distance between learned_representation to give anomaly score to each patch position of the test images\n",
    "    embedding_vectors = all_embedding_vectors[anomaly_class]\n",
    "    B, C, H, W = embedding_vectors.size()\n",
    "    #print('embedding_vectors shape:', embedding_vectors.shape)\n",
    "    \n",
    "    embedding_vectors = embedding_vectors.view(B, C, H * W).numpy()\n",
    "    dist_list = []\n",
    "    for i in range(H * W):\n",
    "        mean = learned_representation[0][:, i]\n",
    "        conv_inv = np.linalg.inv(learned_representation[1][:, :, i])\n",
    "        dist = [mahalanobis(sample[:, i], mean, conv_inv) for sample in embedding_vectors]\n",
    "        dist_list.append(dist)\n",
    "    dist_list = np.array(dist_list).transpose(1, 0).reshape(B, H, W)\n",
    "    \n",
    "    # upsample to image size to get anomaly score map\n",
    "    dist_list = torch.tensor(dist_list)\n",
    "    score_map = F.interpolate(dist_list.unsqueeze(1), size=all_images[anomaly_class][0].shape[2], mode='bilinear',\n",
    "                                align_corners=False).squeeze().numpy()\n",
    "    \n",
    "    # apply gaussian smoothing on the score map\n",
    "    for i in range(score_map.shape[0]):\n",
    "        score_map[i] = gaussian_filter(score_map[i], sigma=4)\n",
    "    \n",
    "    # Normalize the score map\n",
    "    max_score = score_map.max()\n",
    "    min_score = score_map.min()\n",
    "    scores = (score_map - min_score) / (max_score - min_score)\n",
    "    all_scores[anomaly_class] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147ef39d-72f7-42bb-b8cb-02b4cc8a0ccc",
   "metadata": {},
   "source": [
    "### Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eb7c66b-ae99-47bd-9cca-0fc6e4132703",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: absent_septum ****************\n",
      "ROCAUC: 0.810\n",
      "AUPRC: 0.488\n",
      "DICE: 0.495\n",
      "******************* DATASET: artefacts ****************\n",
      "ROCAUC: 0.902\n",
      "AUPRC: 0.277\n",
      "DICE: 0.132\n",
      "******************* DATASET: craniatomy ****************\n",
      "ROCAUC: 0.814\n",
      "AUPRC: 0.168\n",
      "DICE: 0.212\n",
      "******************* DATASET: dural ****************\n",
      "ROCAUC: 0.758\n",
      "AUPRC: 0.547\n",
      "DICE: 0.449\n",
      "******************* DATASET: ea_mass ****************\n",
      "ROCAUC: 0.956\n",
      "AUPRC: 0.437\n",
      "DICE: 0.189\n",
      "******************* DATASET: edema ****************\n",
      "ROCAUC: 0.892\n",
      "AUPRC: 0.486\n",
      "DICE: 0.359\n",
      "******************* DATASET: encephalomalacia ****************\n",
      "ROCAUC: 0.956\n",
      "AUPRC: 0.586\n",
      "DICE: 0.267\n",
      "******************* DATASET: enlarged_ventricles ****************\n",
      "ROCAUC: 0.875\n",
      "AUPRC: 0.491\n",
      "DICE: 0.368\n",
      "******************* DATASET: intraventricular ****************\n",
      "ROCAUC: 0.979\n",
      "AUPRC: 0.539\n",
      "DICE: 0.117\n",
      "******************* DATASET: lesions ****************\n",
      "ROCAUC: 0.909\n",
      "AUPRC: 0.286\n",
      "DICE: 0.129\n",
      "******************* DATASET: mass ****************\n",
      "ROCAUC: 0.971\n",
      "AUPRC: 0.474\n",
      "DICE: 0.150\n",
      "******************* DATASET: posttreatment ****************\n",
      "ROCAUC: 0.802\n",
      "AUPRC: 0.279\n",
      "DICE: 0.341\n",
      "******************* DATASET: resection ****************\n",
      "ROCAUC: 0.860\n",
      "AUPRC: 0.319\n",
      "DICE: 0.331\n",
      "******************* DATASET: sinus ****************\n",
      "ROCAUC: 0.784\n",
      "AUPRC: 0.335\n",
      "DICE: 0.426\n",
      "******************* DATASET: wml ****************\n",
      "ROCAUC: 0.703\n",
      "AUPRC: 0.153\n",
      "DICE: 0.367\n",
      "******************* DATASET: other ****************\n",
      "ROCAUC: 0.859\n",
      "AUPRC: 0.459\n",
      "DICE: 0.417\n"
     ]
    }
   ],
   "source": [
    "total_pixel_rocauc = []\n",
    "total_auprc = []\n",
    "total_dice_score =[]\n",
    "for anomaly_class in test_dataloaders.keys():\n",
    "    if anomaly_class == 'normal':\n",
    "        pass\n",
    "    else:\n",
    "        print('******************* DATASET: {} ****************'.format(anomaly_class))\n",
    "        gt_mask = np.asarray(all_pos_masks[anomaly_class])\n",
    "        gt_mask = gt_mask.astype(int)\n",
    "        gt_mask = gt_mask.astype(np.float32)\n",
    "        scores = all_scores[anomaly_class]\n",
    "        \n",
    "        roc_auc = roc_auc_score(gt_mask.flatten(), scores.flatten())\n",
    "        precision, recall, thresholds = precision_recall_curve(gt_mask.flatten(), scores.flatten())\n",
    "        total_pixel_rocauc.append(roc_auc)\n",
    "\n",
    "\n",
    "        ########## calculate optimal threshold #############\n",
    "\n",
    "        a = 2 * precision * recall\n",
    "        b = precision + recall\n",
    "        f1 = np.divide(a, b, out=np.zeros_like(a), where=b != 0)\n",
    "        threshold = thresholds[np.argmax(f1)]\n",
    "        all_thresholds[anomaly_class] = threshold\\\n",
    "        \n",
    "        #print('threshold for masks: %.3f' % (threshold))\n",
    "        pred_mask = scores.copy()\n",
    "        pred_mask[scores > threshold] = 1\n",
    "        pred_mask[scores <= threshold] = 0\n",
    "\n",
    "        ####################################################\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(gt_mask.flatten(), pred_mask.flatten())\n",
    "        pr_auc = auc(recall, precision)\n",
    "        total_auprc.append(pr_auc)\n",
    "        \n",
    "        intersection = np.sum(scores.flatten() * pred_mask.flatten())\n",
    "        dice_score = (2. * intersection) / (np.sum(scores.flatten()) + np.sum(pred_mask.flatten()))\n",
    "        total_dice_score.append(dice_score)\n",
    "        \n",
    "        print('ROCAUC: %.3f' % (roc_auc))\n",
    "        print('AUPRC: %.3f' % (pr_auc))\n",
    "        print('DICE: %.3f' % (dice_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db851304-9e54-4b07-a2ec-e56b545b7309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total AUROC: 0.8644207298490829\n"
     ]
    }
   ],
   "source": [
    "print('total AUROC:', np.mean(total_pixel_rocauc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f065c6e5-faf5-4af8-b331-3d27d386ca56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total AUPRC: 0.3953308950012978\n"
     ]
    }
   ],
   "source": [
    "print('total AUPRC:', np.mean(total_auprc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cc68bc9-ff2c-40a8-b3a5-82d860ae3eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total DICE: 0.2968046507141038\n"
     ]
    }
   ],
   "source": [
    "print('total DICE:', np.mean(total_dice_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531d7f44-2bc7-4c6b-a10c-cfb3ac91ed35",
   "metadata": {},
   "source": [
    "### Save Qualitatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f547ed2-0022-4e4b-ae01-82ec8627b64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: absent_septum ****************\n",
      "saving images to: ./results/resnet18/pictures/absent_septum\n",
      "******************* DATASET: artefacts ****************\n",
      "saving images to: ./results/resnet18/pictures/artefacts\n",
      "******************* DATASET: craniatomy ****************\n",
      "saving images to: ./results/resnet18/pictures/craniatomy\n",
      "******************* DATASET: dural ****************\n",
      "saving images to: ./results/resnet18/pictures/dural\n",
      "******************* DATASET: ea_mass ****************\n",
      "saving images to: ./results/resnet18/pictures/ea_mass\n",
      "******************* DATASET: edema ****************\n",
      "saving images to: ./results/resnet18/pictures/edema\n",
      "******************* DATASET: encephalomalacia ****************\n",
      "saving images to: ./results/resnet18/pictures/encephalomalacia\n",
      "******************* DATASET: enlarged_ventricles ****************\n",
      "saving images to: ./results/resnet18/pictures/enlarged_ventricles\n",
      "******************* DATASET: intraventricular ****************\n",
      "saving images to: ./results/resnet18/pictures/intraventricular\n",
      "******************* DATASET: lesions ****************\n",
      "saving images to: ./results/resnet18/pictures/lesions\n",
      "******************* DATASET: mass ****************\n",
      "saving images to: ./results/resnet18/pictures/mass\n",
      "******************* DATASET: posttreatment ****************\n",
      "saving images to: ./results/resnet18/pictures/posttreatment\n",
      "******************* DATASET: resection ****************\n",
      "saving images to: ./results/resnet18/pictures/resection\n",
      "******************* DATASET: sinus ****************\n",
      "saving images to: ./results/resnet18/pictures/sinus\n",
      "******************* DATASET: wml ****************\n",
      "saving images to: ./results/resnet18/pictures/wml\n",
      "******************* DATASET: other ****************\n",
      "saving images to: ./results/resnet18/pictures/other\n"
     ]
    }
   ],
   "source": [
    "for anomaly_class in test_dataloaders.keys():\n",
    "    if anomaly_class == 'normal':\n",
    "        pass\n",
    "    else:        \n",
    "        print('******************* DATASET: {} ****************'.format(anomaly_class))\n",
    "        images = all_images[anomaly_class]\n",
    "        scores = all_scores[anomaly_class]\n",
    "        masks = all_pos_masks[anomaly_class]\n",
    "        neg_masks = all_neg_masks[anomaly_class]\n",
    "        threshold = all_thresholds[anomaly_class]\n",
    "        save_dir = os.path.join(pic_save_path, anomaly_class)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        print('saving images to:', save_dir)    \n",
    "        visualize_images(images, scores, neg_masks, masks, threshold, save_dir, anomaly_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c38428a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlm",
   "language": "python",
   "name": "adlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
