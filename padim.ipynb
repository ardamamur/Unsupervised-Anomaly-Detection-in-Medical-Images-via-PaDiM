{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "591667fe-8496-444b-b076-ea2cfeb7439c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/usr_mlmi/.conda/envs/ark_biovil/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-01 18:34:06,720] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from random import sample\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from preprocess_data import DataHandler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import wide_resnet50_2, resnet18\n",
    "from fast_ixi import FAST_IXI\n",
    "from utils import *\n",
    "from eval import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f1718b-a3ad-4529-9a6d-16ef5aeefce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# device setup\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "print('Device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9524da8-d71d-4ca6-a696-1c1739486c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "seed = 1024\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5da45b62-d5a4-4d53-ab1e-55e2d74b945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'config.yaml'\n",
    "opt = read_config(config_path)\n",
    "experiment_path = opt['dataset']['save_dir'] + '/' + opt['model']['backbone']\n",
    "opt['model']['experiment_path'] = experiment_path\n",
    "\n",
    "os.makedirs(os.path.join(experiment_path, 'temp_%s' % opt['model']['backbone']), exist_ok=True)\n",
    "train_feature_filepath = os.path.join(experiment_path, 'temp_%s' % opt['model']['backbone'], 'train_%s.pkl' % 'brainmri')\n",
    "opt['model']['train_feature_filepath'] = train_feature_filepath\n",
    "\n",
    "pic_save_path = os.path.join(experiment_path, 'pictures')\n",
    "os.makedirs(pic_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845774d1-b9f8-485f-a3c7-b3ee8c2bac88",
   "metadata": {},
   "source": [
    "### Load pre-trained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23ded272-6ff7-4896-971f-61e8f832ffc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone: wide_resnet50_2\n",
      "Input dim size: 1792\n",
      "Output dim size after reduced: 550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pretrained CNN\n",
    "model, t_d, d = load_pretrained_CNN(opt)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d57085d9-598b-4c5a-9ce1-6b14ae1a949b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f84a68a3940>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select randomly choosen dimension to reduce the dimensionality of the feature vector (like PCA)\n",
    "idx = torch.tensor(sample(range(0, t_d), d))\n",
    "# initialize the intermadiate outputs\n",
    "outputs = []\n",
    "def hook(module, input, output):\n",
    "    outputs.append(output)\n",
    "model.layer1[-1].register_forward_hook(hook)\n",
    "model.layer2[-1].register_forward_hook(hook)\n",
    "model.layer3[-1].register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de9b4cf-197c-4367-929c-460cd1955f42",
   "metadata": {},
   "source": [
    "### Learning Normal Class Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85adc64f-155a-4368-a308-9490dc233918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 581 IXI images and 130 fastMRI images for training. Using 15 images for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guests/usr_mlmi/.conda/envs/ark_biovil/lib/python3.9/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = load_train_dataset(opt).train_dataloader()\n",
    "learned_representation = OrderedDict([('layer1', []), ('layer2', []), ('layer3', [])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a77e7d2-6dfe-4f0a-a377-2f3e122d0f03",
   "metadata": {},
   "source": [
    "#### Extract embeddings fromm the train dataset and save it as Multivariate Gaussian Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "719c93f4-6675-417c-a64e-442dc6cd24f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(train_feature_filepath):    \n",
    "# for each batch in the dataloader (use tqdm bar), train dataloader get item returns only x\n",
    "    for batch_idx, img in tqdm(enumerate(train_dataloader), '| feature extraction | train | %s |' % 'brainmri'):\n",
    "        img = img.to(device)\n",
    "        with torch.no_grad():\n",
    "            _ = model(img)\n",
    "        for key, value in zip(learned_representation.keys(), outputs):\n",
    "            learned_representation[key].append(value.cpu().detach())\n",
    "        # initialize hook outputs\n",
    "        outputs = []\n",
    "\n",
    "    for key, value in learned_representation.items():\n",
    "        learned_representation[key] = torch.cat(value, 0)\n",
    "\n",
    "    print('first layer shape:', learned_representation['layer1'].shape)\n",
    "    print('second layer shape:', learned_representation['layer2'].shape)\n",
    "    print('third layer shape:', learned_representation['layer3'].shape)\n",
    "    # Embedding concat\n",
    "    embedding_vectors = learned_representation['layer1'] # get the maximum size of the embedding vectors\n",
    "    \"\"\"\n",
    "    Rresearchers conceptually divide the input image into a grid based on the resolution of the largest activation map—typically\n",
    "    the first layer of the pre-trained CNN. This way, each grid position, denoted as (i,j), \n",
    "    is associated with a unique embedding vector that represents the collective activation vectors for that particular image patch.\n",
    "    \"\"\"\n",
    "    for layer_name in ['layer2', 'layer3']:\n",
    "        embedding_vectors = embedding_concat(embedding_vectors, learned_representation[layer_name])\n",
    "\n",
    "    # randomly select d dimension\n",
    "    print('randomly select %d dimension' % opt['model']['output_dimension'])\n",
    "    embedding_vectors = torch.index_select(embedding_vectors, 1, idx)\n",
    "\n",
    "    B, C, H, W = embedding_vectors.size() # Get the shape of the embedding vectors which is same with the first layer of the pretrained model\n",
    "    print('embedding_vectors shape:', embedding_vectors.shape)\n",
    "    embedding_vectors = embedding_vectors.view(B, C, H * W)\n",
    "\n",
    "    # calculate multivariate Gaussian distribution\n",
    "    mean = torch.mean(embedding_vectors, dim=0).numpy()\n",
    "    cov = torch.zeros(C, C, H * W).numpy()\n",
    "    I = np.identity(C)\n",
    "\n",
    "    # calculate mean, cov and inverse covariance matrix for each patch position at Xij \n",
    "    # (each patch position (i,j) is associated with a unique embedding vector)\n",
    "    for i in range(H * W):\n",
    "        # Xij = embedding_vectors[:, :, i].numpy()\n",
    "        cov[:, :, i] = np.cov(embedding_vectors[:, :, i].numpy(), rowvar=False) + 0.01 * I\n",
    "\n",
    "    # save learned distribution\n",
    "    learned_representation = [mean, cov]\n",
    "    with open(train_feature_filepath, 'wb') as f:\n",
    "        pickle.dump(learned_representation, f)\n",
    "else:\n",
    "    with open(train_feature_filepath, 'rb') as f:\n",
    "        learned_representation = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899a9aa-d308-4850-94fa-cf3ddd57746f",
   "metadata": {},
   "source": [
    "### Evaluate on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd39671a-2648-4677-8212-6c319030f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import get_all_test_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e21e81c7-945e-4282-951c-1833eac89c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloaders = get_all_test_dataloaders( opt['dataset']['ann_path'],  opt['dataset']['target_size'], opt['dataset']['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67bc7be3-aa1f-40d3-8597-db0e013d46ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of anomaly classes: 17\n"
     ]
    }
   ],
   "source": [
    "print('number of anomaly classes:', len(test_dataloaders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0c54767-31c6-4fbd-8eb9-3ba8f456a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = {key : [] for key in test_dataloaders.keys()}\n",
    "all_labels = {key : [] for key in test_dataloaders.keys()}\n",
    "all_pos_masks = {key : [] for key in test_dataloaders.keys()}\n",
    "all_neg_masks = {key : [] for key in test_dataloaders.keys()}\n",
    "all_thresholds = {key : [] for key in test_dataloaders.keys()}\n",
    "all_test_outputs = { key: OrderedDict([('layer1', []), ('layer2', []), ('layer3', [])]) for key in test_dataloaders.keys()}\n",
    "all_embedding_vectors = {key : [] for key in test_dataloaders.keys()}\n",
    "all_scores = {key : [] for key in test_dataloaders.keys()}\n",
    "\n",
    "extend_images = []\n",
    "extend_labels = []\n",
    "extend_pos_masks = []\n",
    "extend_neg_masks = []\n",
    "extend_scores = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17661861-9e5c-4484-967f-3a4f0e85a1a3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: absent_septum ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | absent_septum |:   0%|                                                                         | 0/1 [00:00<?, ?it/s]/home/guests/usr_mlmi/.conda/envs/ark_biovil/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "| feature extraction | test | absent_septum |: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 550, 32, 32])\n",
      "******************* DATASET: artefacts ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | artefacts |: 100%|███████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 45.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 550, 32, 32])\n",
      "******************* DATASET: craniatomy ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | craniatomy |: 100%|██████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 47.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 550, 32, 32])\n",
      "******************* DATASET: dural ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | dural |: 100%|█████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 49.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 550, 32, 32])\n",
      "******************* DATASET: ea_mass ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | ea_mass |: 100%|███████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 49.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 550, 32, 32])\n",
      "******************* DATASET: edema ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | edema |: 100%|███████████████████████████████████████████████████████████████████████| 18/18 [00:00<00:00, 48.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 550, 32, 32])\n",
      "******************* DATASET: encephalomalacia ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | encephalomalacia |: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 47.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 550, 32, 32])\n",
      "******************* DATASET: enlarged_ventricles ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | enlarged_ventricles |: 100%|█████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 49.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19, 550, 32, 32])\n",
      "******************* DATASET: intraventricular ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | intraventricular |: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 41.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 550, 32, 32])\n",
      "******************* DATASET: lesions ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | lesions |: 100%|█████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 49.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 550, 32, 32])\n",
      "******************* DATASET: mass ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | mass |: 100%|████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 50.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 550, 32, 32])\n",
      "******************* DATASET: posttreatment ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | posttreatment |: 100%|███████████████████████████████████████████████████████████████| 44/44 [00:00<00:00, 49.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([44, 550, 32, 32])\n",
      "******************* DATASET: resection ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | resection |: 100%|███████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 51.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 550, 32, 32])\n",
      "******************* DATASET: sinus ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | sinus |: 100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 51.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 550, 32, 32])\n",
      "******************* DATASET: wml ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | wml |: 100%|███████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 48.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 550, 32, 32])\n",
      "******************* DATASET: other ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | other |: 100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 53.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 550, 32, 32])\n",
      "******************* DATASET: normal ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | normal |: 100%|██████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 50.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 550, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for anomaly_class in test_dataloaders.keys():\n",
    "    print('******************* DATASET: {} ****************'.format(anomaly_class)) \n",
    "    imgs = []\n",
    "    labels = []\n",
    "    pos_masks = []\n",
    "    neg_masks = []\n",
    "    for (img, label, pos_mask, neg_mask) in tqdm(test_dataloaders[anomaly_class], '| feature extraction | test | %s |' % anomaly_class):\n",
    "        imgs.extend(img.cpu().detach().numpy())\n",
    "        labels.extend(label.cpu().detach().numpy())\n",
    "        pos_masks.extend(pos_mask.cpu().detach().numpy())\n",
    "        neg_masks.extend(neg_mask.cpu().detach().numpy())\n",
    "        extend_images.extend(img.cpu().detach().numpy())\n",
    "        extend_labels.extend(label.cpu().detach().numpy())\n",
    "        extend_pos_masks.extend(pos_mask.cpu().detach().numpy())\n",
    "        extend_neg_masks.extend(neg_mask.cpu().detach().numpy())\n",
    "\n",
    "        # get the model prediction\n",
    "        with torch.no_grad():\n",
    "            _ = model(img.to(device))\n",
    "        # get intermediate outputs\n",
    "        for key, value in zip(all_test_outputs[anomaly_class].keys(), outputs):\n",
    "            all_test_outputs[anomaly_class][key].append(value.cpu().detach())\n",
    "            \n",
    "    # initialize hook outputs\n",
    "    outputs = []\n",
    "    for key, value in all_test_outputs[anomaly_class].items():\n",
    "        all_test_outputs[anomaly_class][key] = torch.cat(value, 0)\n",
    "    # Embedding concat\n",
    "    embedding_vectors = all_test_outputs[anomaly_class]['layer1']\n",
    "    for layer_name in ['layer2', 'layer3']:\n",
    "        embedding_vectors = embedding_concat(embedding_vectors, all_test_outputs[anomaly_class][layer_name])\n",
    "    \n",
    "    # randomly select d dimension\n",
    "    embedding_vectors = torch.index_select(embedding_vectors, 1, idx)\n",
    "    print(embedding_vectors.shape)\n",
    "    \n",
    "\n",
    "    all_images[anomaly_class] = imgs\n",
    "    all_labels[anomaly_class] = labels\n",
    "    all_pos_masks[anomaly_class] = pos_masks\n",
    "    all_neg_masks[anomaly_class] = neg_masks\n",
    "    all_embedding_vectors[anomaly_class] = embedding_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45310ca7-e85e-4985-b27f-83599b8dcdec",
   "metadata": {},
   "source": [
    "#### Get Anomaly Map for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e5e97cb-a1b7-46af-bfa2-a780039fb284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: absent_septum ****************\n",
      "embedding_vectors shape: torch.Size([1, 550, 32, 32])\n",
      "******************* DATASET: artefacts ****************\n",
      "embedding_vectors shape: torch.Size([16, 550, 32, 32])\n",
      "******************* DATASET: craniatomy ****************\n",
      "embedding_vectors shape: torch.Size([15, 550, 32, 32])\n",
      "******************* DATASET: dural ****************\n",
      "embedding_vectors shape: torch.Size([7, 550, 32, 32])\n",
      "******************* DATASET: ea_mass ****************\n",
      "embedding_vectors shape: torch.Size([4, 550, 32, 32])\n",
      "******************* DATASET: edema ****************\n",
      "embedding_vectors shape: torch.Size([18, 550, 32, 32])\n",
      "******************* DATASET: encephalomalacia ****************\n",
      "embedding_vectors shape: torch.Size([1, 550, 32, 32])\n",
      "******************* DATASET: enlarged_ventricles ****************\n",
      "embedding_vectors shape: torch.Size([19, 550, 32, 32])\n",
      "******************* DATASET: intraventricular ****************\n",
      "embedding_vectors shape: torch.Size([1, 550, 32, 32])\n",
      "******************* DATASET: lesions ****************\n",
      "embedding_vectors shape: torch.Size([22, 550, 32, 32])\n",
      "******************* DATASET: mass ****************\n",
      "embedding_vectors shape: torch.Size([22, 550, 32, 32])\n",
      "******************* DATASET: posttreatment ****************\n",
      "embedding_vectors shape: torch.Size([44, 550, 32, 32])\n",
      "******************* DATASET: resection ****************\n",
      "embedding_vectors shape: torch.Size([10, 550, 32, 32])\n",
      "******************* DATASET: sinus ****************\n",
      "embedding_vectors shape: torch.Size([2, 550, 32, 32])\n",
      "******************* DATASET: wml ****************\n",
      "embedding_vectors shape: torch.Size([5, 550, 32, 32])\n",
      "******************* DATASET: other ****************\n",
      "embedding_vectors shape: torch.Size([5, 550, 32, 32])\n",
      "******************* DATASET: normal ****************\n",
      "embedding_vectors shape: torch.Size([30, 550, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for anomaly_class in test_dataloaders.keys():\n",
    "    print('******************* DATASET: {} ****************'.format(anomaly_class)) \n",
    "    # calculate mahalanobis distance between learned_representation to give anomaly score to each patch position of the test images\n",
    "    embedding_vectors = all_embedding_vectors[anomaly_class]\n",
    "    B, C, H, W = embedding_vectors.size()\n",
    "    print('embedding_vectors shape:', embedding_vectors.shape)\n",
    "    \n",
    "    embedding_vectors = embedding_vectors.view(B, C, H * W).numpy()\n",
    "    dist_list = []\n",
    "    for i in range(H * W):\n",
    "        mean = learned_representation[0][:, i]\n",
    "        conv_inv = np.linalg.inv(learned_representation[1][:, :, i])\n",
    "        dist = [mahalanobis(sample[:, i], mean, conv_inv) for sample in embedding_vectors]\n",
    "        dist_list.append(dist)\n",
    "    dist_list = np.array(dist_list).transpose(1, 0).reshape(B, H, W)\n",
    "    \n",
    "    # upsample to image size to get anomaly score map\n",
    "    dist_list = torch.tensor(dist_list)\n",
    "    score_map = F.interpolate(dist_list.unsqueeze(1), size=all_images[anomaly_class][0].shape[2], mode='bilinear',\n",
    "                                align_corners=False).squeeze().numpy()\n",
    "    \n",
    "    # apply gaussian smoothing on the score map\n",
    "    for i in range(score_map.shape[0]):\n",
    "        score_map[i] = gaussian_filter(score_map[i], sigma=4)\n",
    "    \n",
    "    # Normalize the score map\n",
    "    max_score = score_map.max()\n",
    "    min_score = score_map.min()\n",
    "    scores = (score_map - min_score) / (max_score - min_score)\n",
    "    all_scores[anomaly_class] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147ef39d-72f7-42bb-b8cb-02b4cc8a0ccc",
   "metadata": {},
   "source": [
    "### Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eb7c66b-ae99-47bd-9cca-0fc6e4132703",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: absent_septum ****************\n",
      "threshold for masks: 0.512\n",
      "pixel ROCAUC: 0.810\n",
      "pixel AUPRC: 0.456\n",
      "pixel DICE: 0.511\n",
      "******************* DATASET: artefacts ****************\n",
      "threshold for masks: 0.294\n",
      "pixel ROCAUC: 0.713\n",
      "pixel AUPRC: 0.363\n",
      "pixel DICE: 0.455\n",
      "******************* DATASET: craniatomy ****************\n",
      "threshold for masks: 0.446\n",
      "pixel ROCAUC: 0.587\n",
      "pixel AUPRC: 0.447\n",
      "pixel DICE: 0.756\n",
      "******************* DATASET: dural ****************\n",
      "threshold for masks: 0.471\n",
      "pixel ROCAUC: 0.740\n",
      "pixel AUPRC: 0.476\n",
      "pixel DICE: 0.581\n",
      "******************* DATASET: ea_mass ****************\n",
      "threshold for masks: 0.331\n",
      "pixel ROCAUC: 0.523\n",
      "pixel AUPRC: 0.479\n",
      "pixel DICE: 0.717\n",
      "******************* DATASET: edema ****************\n",
      "threshold for masks: 0.478\n",
      "pixel ROCAUC: 0.727\n",
      "pixel AUPRC: 0.524\n",
      "pixel DICE: 0.642\n",
      "******************* DATASET: encephalomalacia ****************\n",
      "threshold for masks: 0.753\n",
      "pixel ROCAUC: 0.950\n",
      "pixel AUPRC: 0.596\n",
      "pixel DICE: 0.252\n",
      "******************* DATASET: enlarged_ventricles ****************\n",
      "threshold for masks: 0.694\n",
      "pixel ROCAUC: 0.834\n",
      "pixel AUPRC: 0.601\n",
      "pixel DICE: 0.655\n",
      "******************* DATASET: intraventricular ****************\n",
      "threshold for masks: 0.842\n",
      "pixel ROCAUC: 0.967\n",
      "pixel AUPRC: 0.490\n",
      "pixel DICE: 0.176\n",
      "******************* DATASET: lesions ****************\n",
      "threshold for masks: 0.794\n",
      "pixel ROCAUC: 0.845\n",
      "pixel AUPRC: 0.242\n",
      "pixel DICE: 0.301\n",
      "******************* DATASET: mass ****************\n",
      "threshold for masks: 0.735\n",
      "pixel ROCAUC: 0.778\n",
      "pixel AUPRC: 0.237\n",
      "pixel DICE: 0.306\n",
      "******************* DATASET: posttreatment ****************\n",
      "threshold for masks: 0.666\n",
      "pixel ROCAUC: 0.609\n",
      "pixel AUPRC: 0.129\n",
      "pixel DICE: 0.393\n",
      "******************* DATASET: resection ****************\n",
      "threshold for masks: 0.727\n",
      "pixel ROCAUC: 0.800\n",
      "pixel AUPRC: 0.262\n",
      "pixel DICE: 0.291\n",
      "******************* DATASET: sinus ****************\n",
      "threshold for masks: 0.572\n",
      "pixel ROCAUC: 0.793\n",
      "pixel AUPRC: 0.469\n",
      "pixel DICE: 0.499\n",
      "******************* DATASET: wml ****************\n",
      "threshold for masks: 0.723\n",
      "pixel ROCAUC: 0.805\n",
      "pixel AUPRC: 0.354\n",
      "pixel DICE: 0.477\n",
      "******************* DATASET: other ****************\n",
      "threshold for masks: 0.385\n",
      "pixel ROCAUC: 0.580\n",
      "pixel AUPRC: 0.365\n",
      "pixel DICE: 0.628\n"
     ]
    }
   ],
   "source": [
    "total_pixel_rocauc = []\n",
    "total_auprc = []\n",
    "total_dice_score =[]\n",
    "for anomaly_class in test_dataloaders.keys():\n",
    "    if anomaly_class == 'normal':\n",
    "        pass\n",
    "    else:\n",
    "        print('******************* DATASET: {} ****************'.format(anomaly_class))\n",
    "        gt_mask = np.asarray(all_pos_masks[anomaly_class])\n",
    "        gt_mask = gt_mask.astype(int)\n",
    "        gt_mask = gt_mask.astype(np.float32)\n",
    "        scores = all_scores[anomaly_class]\n",
    "        \n",
    "        roc_auc = roc_auc_score(gt_mask.flatten(), scores.flatten())\n",
    "        precision, recall, thresholds = precision_recall_curve(gt_mask.flatten(), scores.flatten())\n",
    "        total_pixel_rocauc.append(roc_auc)\n",
    "\n",
    "\n",
    "        ########## calculate optimal threshold #############\n",
    "\n",
    "        a = 2 * precision * recall\n",
    "        b = precision + recall\n",
    "        f1 = np.divide(a, b, out=np.zeros_like(a), where=b != 0)\n",
    "        threshold = thresholds[np.argmax(f1)]\n",
    "        all_thresholds[anomaly_class] = threshold\\\n",
    "        \n",
    "        print('threshold for masks: %.3f' % (threshold))\n",
    "        pred_mask = scores.copy()\n",
    "        pred_mask[pred_mask > threshold] = 1\n",
    "        pred_mask[pred_mask <= threshold] = 0\n",
    "\n",
    "        ####################################################\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(gt_mask.flatten(), pred_mask.flatten())\n",
    "        pr_auc = auc(recall, precision)\n",
    "        total_auprc.append(pr_auc)\n",
    "        \n",
    "        intersection = np.sum(scores.flatten() * pred_mask.flatten())\n",
    "        dice_score = (2. * intersection) / (np.sum(scores.flatten()) + np.sum(pred_mask.flatten()))\n",
    "        total_dice_score.append(dice_score)\n",
    "        \n",
    "        print('pixel ROCAUC: %.3f' % (roc_auc))\n",
    "        print('pixel AUPRC: %.3f' % (pr_auc))\n",
    "        print('pixel DICE: %.3f' % (dice_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db851304-9e54-4b07-a2ec-e56b545b7309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total AUROC: 0.7537521874199522\n"
     ]
    }
   ],
   "source": [
    "print('total AUROC:', np.mean(total_pixel_rocauc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f065c6e5-faf5-4af8-b331-3d27d386ca56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total AUPRC: 0.4056331415630455\n"
     ]
    }
   ],
   "source": [
    "print('total AUPRC:', np.mean(total_auprc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cc68bc9-ff2c-40a8-b3a5-82d860ae3eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total DICE: 0.47759564603874627\n"
     ]
    }
   ],
   "source": [
    "print('total DICE:', np.mean(total_dice_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531d7f44-2bc7-4c6b-a10c-cfb3ac91ed35",
   "metadata": {},
   "source": [
    "### Save Qualitatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd84a095-3f61-40d7-ad47-30d760c76b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f547ed2-0022-4e4b-ae01-82ec8627b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for anomaly_class in test_dataloaders.keys():\n",
    "#     if anomaly_class == 'normal':\n",
    "#         pass\n",
    "#     elif len(all_images[anomaly_class]) == 1:\n",
    "#         pass\n",
    "#     else:\n",
    "#         print('******************* DATASET: {} ****************'.format(anomaly_class))\n",
    "#         images = all_images[anomaly_class]\n",
    "#         scores = all_scores[anomaly_class]\n",
    "#         masks = all_pos_masks[anomaly_class]\n",
    "#         neg_masks = all_neg_masks[anomaly_class]\n",
    "#         threshold = all_thresholds[anomaly_class]\n",
    "#         save_dir = os.path.join(experiment_path, anomaly_class)\n",
    "#         os.makedirs(save_dir, exist_ok=True)\n",
    "#         plot_fig(images, scores, neg_masks, masks, threshold, save_dir, anomaly_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586838e5-fd3f-4ad2-b059-1018d77cb454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ark_biovil",
   "language": "python",
   "name": "ark_biovil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
