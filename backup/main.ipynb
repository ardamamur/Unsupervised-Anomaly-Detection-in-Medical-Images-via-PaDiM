{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import sample\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from utils import *\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# device setup\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "print('Device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'config.yaml'\n",
    "opt = read_config(config_path)\n",
    "experiment_path = opt['dataset']['save_dir'] + '/' + opt['model']['backbone']\n",
    "opt['model']['experiment_path'] = experiment_path\n",
    "\n",
    "os.makedirs(os.path.join(experiment_path, 'temp_%s' % opt['model']['backbone']), exist_ok=True)\n",
    "train_feature_filepath = os.path.join(experiment_path, 'temp_%s' % opt['model']['backbone'], 'train_%s.pkl' % 'brainmri')\n",
    "opt['model']['train_feature_filepath'] = train_feature_filepath\n",
    "\n",
    "\n",
    "pic_save_path = os.path.join(experiment_path, 'pictures')\n",
    "os.makedirs(pic_save_path, exist_ok=True)\n",
    "\n",
    "# set random seed\n",
    "seed = opt['model']['seed']\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-trained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone: resnet18\n",
      "Input dim size: 448\n",
      "Output dim size after reduced: 180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mamur/miniconda3/envs/adlm/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mamur/miniconda3/envs/adlm/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pretrained CNN\n",
    "model, t_d, r_d = load_pretrained_CNN(opt)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f30f0d21d00>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select randomly choosen dimension to reduce the dimensionality of the feature vector (like PCA)\n",
    "idx = torch.tensor(sample(range(0, t_d), r_d))\n",
    "outputs = []\n",
    "def hook(module, input, output):\n",
    "    outputs.append(output)\n",
    "model.layer1[-1].register_forward_hook(hook)\n",
    "model.layer2[-1].register_forward_hook(hook)\n",
    "model.layer3[-1].register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 581 IXI images and 130 fastMRI images for training. Using 15 images for validation.\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = load_train_dataset(opt).train_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Class Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outputs = OrderedDict([('layer1', []), ('layer2', []), ('layer3', [])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(train_feature_filepath):    \n",
    "# for each batch in the dataloader (use tqdm bar), train dataloader get item returns only x\n",
    "    for batch_idx, img in tqdm(enumerate(train_dataloader), '| feature extraction | train | %s |' % 'brainmri'):\n",
    "        img = img.to(device)\n",
    "        with torch.no_grad():\n",
    "            _ = model(img)\n",
    "        for key, value in zip(train_outputs.keys(), outputs):\n",
    "            train_outputs[key].append(value.cpu().detach())\n",
    "        # initialize hook outputs\n",
    "        outputs = []\n",
    "\n",
    "    for key, value in train_outputs.items():\n",
    "        train_outputs[key] = torch.cat(value, 0)\n",
    "\n",
    "    print('first layer shape:', train_outputs['layer1'].shape)\n",
    "    print('second layer shape:', train_outputs['layer2'].shape)\n",
    "    print('third layer shape:', train_outputs['layer3'].shape)\n",
    "    # Embedding concat\n",
    "    embedding_vectors = train_outputs['layer1'] # get the maximum size of the embedding vectors\n",
    "    \"\"\"\n",
    "    Rresearchers conceptually divide the input image into a grid based on the resolution of the largest activation map—typically\n",
    "    the first layer of the pre-trained CNN. This way, each grid position, denoted as (i,j), \n",
    "    is associated with a unique embedding vector that represents the collective activation vectors for that particular image patch.\n",
    "    \"\"\"\n",
    "    for layer_name in ['layer2', 'layer3']:\n",
    "        embedding_vectors = embedding_concat(embedding_vectors, train_outputs[layer_name])\n",
    "\n",
    "    # randomly select d dimension\n",
    "    print('randomly select %d dimension' % opt['model']['output_dimension'])\n",
    "    embedding_vectors = torch.index_select(embedding_vectors, 1, idx)\n",
    "\n",
    "    B, C, H, W = embedding_vectors.size() # Get the shape of the embedding vectors which is same with the first layer of the pretrained model\n",
    "    print('embedding_vectors shape:', embedding_vectors.shape)\n",
    "    embedding_vectors = embedding_vectors.view(B, C, H * W)\n",
    "\n",
    "    # calculate multivariate Gaussian distribution\n",
    "    mean = torch.mean(embedding_vectors, dim=0).numpy()\n",
    "    cov = torch.zeros(C, C, H * W).numpy()\n",
    "    I = np.identity(C)\n",
    "\n",
    "    # calculate mean, cov and inverse covariance matrix for each patch position at Xij \n",
    "    # (each patch position (i,j) is associated with a unique embedding vector)\n",
    "    for i in range(H * W):\n",
    "        # Xij = embedding_vectors[:, :, i].numpy()\n",
    "        cov[:, :, i] = np.cov(embedding_vectors[:, :, i].numpy(), rowvar=False) + 0.01 * I\n",
    "\n",
    "    # save learned distribution\n",
    "    train_outputs = [mean, cov]\n",
    "    with open(train_feature_filepath, 'wb') as f:\n",
    "        pickle.dump(train_outputs, f)\n",
    "else:\n",
    "    with open(train_feature_filepath, 'rb') as f:\n",
    "        train_outputs = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PaDiM at Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import get_all_test_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloaders = get_all_test_dataloaders( opt['dataset']['ann_path'],  opt['dataset']['target_size'], opt['dataset']['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs_dict = {key: OrderedDict([('layer1', []), ('layer2', []), ('layer3', [])]) for key in test_dataloaders.keys()}\n",
    "metrics = {\n",
    "    'TP': [],\n",
    "    'FP': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'ROCAUC' : [],\n",
    "    'PRO-SCORE': [],\n",
    "}\n",
    "all_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* DATASET: absent_septum ****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| feature extraction | test | absent_septum | brainmri |: 1it [00:00, 111.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first layer shape: torch.Size([1, 64, 32, 32])\n",
      "second layer shape: torch.Size([1, 128, 16, 16])\n",
      "third layer shape: torch.Size([1, 256, 8, 8])\n",
      "embedding_vectors shape: torch.Size([1, 180, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(H \u001b[38;5;241m*\u001b[39m W):\n\u001b[1;32m     58\u001b[0m     mean \u001b[38;5;241m=\u001b[39m train_outputs[\u001b[38;5;241m0\u001b[39m][:, i]\n\u001b[0;32m---> 59\u001b[0m     conv_inv \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     dist \u001b[38;5;241m=\u001b[39m [mahalanobis(sample[:, i], mean, conv_inv) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m embedding_vectors]\n\u001b[1;32m     61\u001b[0m     dist_list\u001b[38;5;241m.\u001b[39mappend(dist)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/adlm/lib/python3.9/site-packages/numpy/linalg/linalg.py:538\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    536\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    537\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 538\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for dataset_key in test_dataloaders.keys():\n",
    "\n",
    "    test_dataloader = test_dataloaders[dataset_key]\n",
    "    test_metrics = {\n",
    "        'TP': [],\n",
    "        'FP': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1': [],\n",
    "        'ROCAUC' : [],\n",
    "        'PRO-SCORE': [],\n",
    "    }\n",
    "    tps, fns, fps = 0, 0, []\n",
    "\n",
    "    print('******************* DATASET: {} ****************'.format(dataset_key))\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    masks = []\n",
    "    neg_masks = []\n",
    "\n",
    "    for batch_idx, data in tqdm(enumerate(test_dataloader), '| feature extraction | test | %s | %s |' % (dataset_key, 'brainmri')):\n",
    "        img, label, mask, neg_mask = data\n",
    "        images.append(img[0].cpu().detach().numpy())\n",
    "        labels.append(label[0].cpu().detach().numpy())\n",
    "        masks.append(mask[0].cpu().detach().numpy())\n",
    "        neg_masks.append(neg_mask[0].cpu().detach().numpy())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _ = model(img.to(device))\n",
    "\n",
    "        for key, value in zip(test_outputs_dict[dataset_key].keys(), outputs):\n",
    "            test_outputs_dict[dataset_key][key].append(value.cpu().detach())\n",
    "        \n",
    "        # initialize hook outputs   \n",
    "        outputs = []\n",
    "\n",
    "    for key, value in test_outputs_dict[dataset_key].items():\n",
    "        test_outputs_dict[dataset_key][key] = torch.cat(value, 0)\n",
    "\n",
    "    print('first layer shape:', test_outputs_dict[dataset_key]['layer1'].shape)\n",
    "    print('second layer shape:', test_outputs_dict[dataset_key]['layer2'].shape)\n",
    "    print('third layer shape:', test_outputs_dict[dataset_key]['layer3'].shape)\n",
    "\n",
    "    # Embedding concat\n",
    "    embedding_vectors = test_outputs_dict[dataset_key]['layer1'] # get the maximum size of the embedding vectors\n",
    "    for layer_name in ['layer2', 'layer3']:\n",
    "        embedding_vectors = embedding_concat(embedding_vectors, test_outputs_dict[dataset_key][layer_name])\n",
    "\n",
    "    embedding_vectors = torch.index_select(embedding_vectors, 1, idx)\n",
    "    # calculate mahalanobis distance between train_outputs to give anomaly score to each patch position of the test images\n",
    "    B, C, H, W = embedding_vectors.size()\n",
    "    print('embedding_vectors shape:', embedding_vectors.shape)\n",
    "\n",
    "    embedding_vectors = embedding_vectors.view(B, C, H * W).numpy()\n",
    "    dist_list = []\n",
    "    for i in range(H * W):\n",
    "        mean = train_outputs[0][:, i]\n",
    "        conv_inv = np.linalg.inv(train_outputs[1][:, :, i])\n",
    "        dist = [mahalanobis(sample[:, i], mean, conv_inv) for sample in embedding_vectors]\n",
    "        dist_list.append(dist)\n",
    "    dist_list = np.array(dist_list).transpose(1, 0).reshape(B, H, W)\n",
    "\n",
    "    # upsample to image size to get anomaly score map\n",
    "    dist_list = torch.tensor(dist_list)\n",
    "    score_map = F.interpolate(dist_list.unsqueeze(1), size=img.size(2), mode='bilinear',\n",
    "                                align_corners=False).squeeze().numpy()\n",
    "\n",
    "    # apply gaussian smoothing on the score map\n",
    "    for i in range(score_map.shape[0]):\n",
    "        score_map[i] = gaussian_filter(score_map[i], sigma=4)\n",
    "\n",
    "    # Normalize the score map\n",
    "    max_score = score_map.max()\n",
    "    min_score = score_map.min()\n",
    "    scores = (score_map - min_score) / (max_score - min_score)\n",
    "\n",
    "    \n",
    "    num = len(images)\n",
    "    vmax = scores.max() * 255.\n",
    "    vmin = scores.min() * 255.\n",
    "\n",
    "    for i in range(num):\n",
    "        img = images[i]\n",
    "        #img = denormalization(img)\n",
    "        img = img.transpose(1, 2, 0)\n",
    "        gt = masks[i].transpose(1, 2, 0).squeeze()\n",
    "        gt_neg = neg_masks[i].transpose(1, 2, 0).squeeze()\n",
    "        anno_map = scores[i]\n",
    "        heat_map = scores[i] * 255\n",
    "        mask = scores[i]\n",
    "        mask[mask > 0.246] = 1\n",
    "        mask[mask <= 0.246] = 0\n",
    "        kernel = morphology.disk(4)\n",
    "        mask = morphology.opening(mask, kernel)\n",
    "        mask *= 255\n",
    "        vis_img = mark_boundaries(img, mask, color=(1, 0, 0), mode='thick')\n",
    "        fig_img, ax_img = plt.subplots(1, 6, gridspec_kw={'wspace': 0, 'hspace': 0})\n",
    "        fig_img.set_size_inches(6 * 4, 4)\n",
    "        norm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "        bboxes = cv2.cvtColor(gt_neg * 255, cv2.COLOR_GRAY2RGB)\n",
    "        cnts_gt = cv2.findContours((gt * 255).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts_gt = cnts_gt[0] if len(cnts_gt) == 2 else cnts_gt[1]\n",
    "        gt_box = []\n",
    "\n",
    "        for c_gt in cnts_gt:\n",
    "            x, y, w, h = cv2.boundingRect(c_gt)\n",
    "            gt_box.append([x, y, x + w, y + h])\n",
    "            cv2.rectangle(bboxes, (x, y), (x + w, y + h), (0, 255, 0), 1)\n",
    "                \n",
    "        x_pos = anno_map * gt\n",
    "        x_neg = anno_map * gt_neg\n",
    "        res_anomaly = np.sum(x_pos)\n",
    "        res_healthy = np.sum(x_neg)\n",
    "\n",
    "\n",
    "        amount_anomaly = np.count_nonzero(x_pos)\n",
    "        amount_mask = np.count_nonzero(gt)\n",
    "\n",
    "        tp = 1 if amount_anomaly > 0.1 * amount_mask else 0  # 10% overlap due to large bboxes e.g. for enlarged ventricles\n",
    "        fn = 1 if tp == 0 else 0\n",
    "        fp = int(res_healthy / max(res_anomaly, 1))\n",
    "        precision = tp / max((tp + fp), 1)\n",
    "\n",
    "        fp = int(res_healthy / max(res_anomaly, 1))\n",
    "        fps.append(fp)\n",
    "        precision = tp / max((tp + fp), 1)\n",
    "        test_metrics['TP'].append(tp)\n",
    "        test_metrics['FP'].append(fp)\n",
    "        test_metrics['Precision'].append(precision)\n",
    "        test_metrics['Recall'].append(tp)\n",
    "        test_metrics['F1'].append(2 * (precision * tp) / (precision + tp + 1e-8))\n",
    "\n",
    "        \n",
    "        for ax_i in ax_img:\n",
    "            ax_i.axes.xaxis.set_visible(False)\n",
    "            ax_i.axes.yaxis.set_visible(False)\n",
    "        ax_img[0].imshow(img)\n",
    "        ax_img[0].title.set_text('Input')\n",
    "\n",
    "        ax_img[1].imshow(vis_img)\n",
    "        ax_img[1].title.set_text('Localization')\n",
    "\n",
    "        \n",
    "        ax_img[2].imshow(bboxes.astype(np.int64), cmap='gray')\n",
    "        ax_img[2].title.set_text('GT')\n",
    "\n",
    "        \n",
    "        ax = ax_img[3].imshow(heat_map, cmap='jet', norm=norm)\n",
    "        ax_img[3].imshow(img, cmap='gray', interpolation='none')\n",
    "        ax_img[3].imshow(heat_map, cmap='jet', alpha=0.5, interpolation='none')\n",
    "        ax_img[3].title.set_text('Anomaly Map')\n",
    "\n",
    "        \n",
    "        ax_img[4].imshow(x_pos, cmap='gray')\n",
    "        ax_img[4].title.set_text(str(np.round(res_anomaly, 2)) + ', TP: ' + str(tp))\n",
    "\n",
    "        \n",
    "        ax_img[5].imshow(x_neg, cmap='gray')\n",
    "        ax_img[5].title.set_text(str(np.round(res_healthy, 2)) + ', FP: ' + str(tp))\n",
    "        \n",
    "        left = 0.92\n",
    "        bottom = 0.15\n",
    "        width = 0.015\n",
    "        height = 1 - 2 * bottom\n",
    "        rect = [left, bottom, width, height]\n",
    "        cbar_ax = fig_img.add_axes(rect)\n",
    "        cb = plt.colorbar(ax, shrink=0.6, cax=cbar_ax, fraction=0.046)\n",
    "        cb.ax.tick_params(labelsize=8)\n",
    "        font = {\n",
    "            'family': 'serif',\n",
    "            'color': 'black',\n",
    "            'weight': 'normal',\n",
    "            'size': 8,\n",
    "        }\n",
    "        cb.set_label('Anomaly Score', fontdict=font)\n",
    "        save_dir = os.path.join(opt['dataset']['save_dir'], opt['d'], dataset_key)\n",
    "        fig_img.savefig(os.path.join(save_dir, class_name + '_{}'.format(i)), dpi=100)\n",
    "        plt.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
